{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Netwrk from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will be creating Neural Network from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import numpy library. If it is not install then do `pip install numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a one hidden layer network having `n_x` input, `n_h` nodes in the hidden layer and 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight(n_x, n_h):\n",
    "    # weight of the w1 will be (n_h, n_x)\n",
    "    w1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.random.randn(n_h,1)*0.01\n",
    "    w2 = np.random.randn(1, n_h)*0.01\n",
    "    b2 = np.random.randn(1,1)*0.01 \n",
    "\n",
    "    parameters ={\n",
    "        \"w1\":w1,\n",
    "        \"b1\":b1,\n",
    "        \"w2\":w2,\n",
    "        \"b2\":b2\n",
    "    }\n",
    "\n",
    "    return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = initialize_weight(n_x=3,n_h=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mparameters\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parameters' is not defined"
     ]
    }
   ],
   "source": [
    "parameters['w1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-1))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    output = np.maximum(0,x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, parameters):\n",
    "    z1 = np.dot(parameters['w1'],X )+ parameters['b1']\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(parameters['w2'],a1) + parameters['b2']\n",
    "    a2 = sigmoid(z2) \n",
    "    return a2\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_weight(n_x=3,n_h=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'relu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(X, parameters)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_pass\u001b[39m(X, parameters):\n\u001b[0;32m      2\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw1\u001b[39m\u001b[38;5;124m'\u001b[39m],X )\u001b[38;5;241m+\u001b[39m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m     a1 \u001b[38;5;241m=\u001b[39m \u001b[43mrelu\u001b[49m(z1)\n\u001b[0;32m      4\u001b[0m     z2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2\u001b[39m\u001b[38;5;124m'\u001b[39m],a1) \u001b[38;5;241m+\u001b[39m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m     a2 \u001b[38;5;241m=\u001b[39m sigmoid(z2) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'relu' is not defined"
     ]
    }
   ],
   "source": [
    "forward_pass(X= x , parameters= parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets do for `L` layer Deep Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights_deep(layer_list):\n",
    "\n",
    "    #layer list contain the info of neural network\n",
    "    #It can  be like [3,4,5,1]\n",
    "    #this means input is 3, there are two hidden layers with 4 and 5 nodes respectively and 1 output\n",
    "\n",
    "\n",
    "    parameters = {}\n",
    "    no_layers = len(layer_list)\n",
    "\n",
    "    for l in range(1,no_layers):\n",
    "        parameters[\"W\"+str(l)] = np.random.randn(layer_list[l], layer_list[l-1])*0.01\n",
    "        parameters[\"b\"+str(l)] = np.random.randn(layer_list[l],1)*0.01\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = [3,4,5,5,1]\n",
    "parameters = initialize_weights_deep(layer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-4.49527397e-03,  7.71922360e-03,  5.57353125e-03],\n",
       "        [-2.41927815e-05, -4.02019588e-03, -1.21790740e-02],\n",
       "        [-1.53329537e-02,  1.68236342e-03,  3.75056335e-03],\n",
       "        [-1.51854194e-02, -3.63632229e-03, -4.58528780e-03]]),\n",
       " 'b1': array([[-0.01067441],\n",
       "        [ 0.00086366],\n",
       "        [ 0.00921181],\n",
       "        [ 0.00830975]]),\n",
       " 'W2': array([[ 0.006436  ,  0.00749933,  0.00914835, -0.00490199],\n",
       "        [ 0.01742212, -0.00117165,  0.00436634, -0.00317488],\n",
       "        [-0.00286265,  0.00601641, -0.00418517, -0.00948183],\n",
       "        [-0.01419913,  0.00750158, -0.00958718,  0.00074018],\n",
       "        [-0.0142927 , -0.00874294,  0.01319679,  0.00898128]]),\n",
       " 'b2': array([[ 0.00544101],\n",
       "        [-0.01860093],\n",
       "        [-0.00525015],\n",
       "        [ 0.01082627],\n",
       "        [-0.00599038]]),\n",
       " 'W3': array([[ 0.01001241,  0.00266502,  0.00189709, -0.00027285, -0.02047561],\n",
       "        [ 0.00488959,  0.00507551, -0.0003637 , -0.00089703, -0.00613735],\n",
       "        [-0.007175  , -0.02276283, -0.01896068,  0.00517062,  0.01066897],\n",
       "        [ 0.00831455,  0.000146  , -0.00813784, -0.00235156,  0.01437596],\n",
       "        [ 0.0056632 ,  0.0146146 , -0.00609004,  0.00896852,  0.00401895]]),\n",
       " 'b3': array([[ 0.00941044],\n",
       "        [ 0.00402137],\n",
       "        [-0.00035099],\n",
       "        [ 0.02111236],\n",
       "        [-0.01242627]]),\n",
       " 'W4': array([[-0.00822614,  0.008913  ,  0.00354885,  0.00600487,  0.0029562 ]]),\n",
       " 'b4': array([[0.00261639]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = [3,4,5,1]\n",
    "parameters = initialize_weights_deep(layer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_forward(X, parameters):\n",
    "\n",
    "    no_weights = len(parameters)//2\n",
    "    A_temp = X\n",
    "    for l in range(1, no_weights):\n",
    "        z = np.dot(parameters[\"W\"+str(l)],A_temp)\n",
    "        a =  relu(z)\n",
    "        A_temp =a\n",
    "\n",
    "    Z = np.dot(parameters[\"W\"+str(no_weights)],A_temp)\n",
    "    a =  sigmoid(Z)\n",
    "     \n",
    "\n",
    "    A = sigmoid(A_temp)\n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = [3,4,5,6,7,8,9,1]\n",
    "parameters = initialize_weights_deep(layer_list)\n",
    "x = np.random.randn(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'relu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdeep_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m, in \u001b[0;36mdeep_forward\u001b[1;34m(X, parameters)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, no_weights):\n\u001b[0;32m      6\u001b[0m     z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(l)],A_temp)\n\u001b[1;32m----> 7\u001b[0m     a \u001b[38;5;241m=\u001b[39m  \u001b[43mrelu\u001b[49m(z)\n\u001b[0;32m      8\u001b[0m     A_temp \u001b[38;5;241m=\u001b[39ma\n\u001b[0;32m     10\u001b[0m Z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(no_weights)],A_temp)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'relu' is not defined"
     ]
    }
   ],
   "source": [
    "deep_forward(X=x, parameters=parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost Function is` mean square error`\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "where:\n",
    "- $ n $ is the number of data points,\n",
    "- $ y_i $ is the actual value of the $i$-th data point,\n",
    "- $ \\hat{y}_i $ is the predicted value of the $i$-th data point.\n",
    "\n",
    "For a single datapoint (i.e., $ n = 1 $), the formula simplifies to:\n",
    "\n",
    "$$ \\text{MSE} = (y - \\hat{y})^2 $$\n",
    "\n",
    "where:\n",
    "- $ y $ is the actual value,\n",
    "- $ \\hat{y} $ is the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(Y,AL):\n",
    "\n",
    "    # Y is the target\n",
    "    # AL is the predicted output\n",
    "\n",
    "    error = Y-AL\n",
    "\n",
    "    #we will use mean square error\n",
    "\n",
    "    cost = (1/2)*(error)**2\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NeuralNetwork():\n",
    "#     def __init__(self,layer_list = [1,3,1]) -> None:\n",
    "#         self.layer_list = layer_list\n",
    "#         self.parameters = self.initialize_weights_deep(self.layer_list)\n",
    "\n",
    "\n",
    "#     def sigmoid(self,x):\n",
    "#         output = 1/(1+np.exp(-1))\n",
    "#         return output\n",
    "\n",
    "#     def relu(self,x):\n",
    "#         output = np.maximum(0,x)\n",
    "#         return output\n",
    "    \n",
    "#     def initialize_weights_deep(layer_list):\n",
    "#         #layer list contain the info of neural network\n",
    "#         #It can  be like [3,4,5,1]\n",
    "#         #this means input is 3, there are two hidden layers with 4 and 5 nodes respectively and 1 output\n",
    "        \n",
    "#         parameters = {}\n",
    "#         no_layers = len(layer_list)\n",
    "#         for l in range(1,no_layers):\n",
    "#             parameters[\"W\"+str(l)] = np.random.randn(layer_list[l], layer_list[l-1])*0.01\n",
    "#             parameters[\"b\"+str(l)] = np.random.randn(layer_list[l],1)*0.01\n",
    "            \n",
    "#         return parameters\n",
    "    \n",
    "#     def deep_forward(self,X):\n",
    "#         no_weights = len(self.parameters)//2\n",
    "#         A_temp = X\n",
    "#         for l in range(1, no_weights):\n",
    "#             z = np.dot(self.parameters[\"W\"+str(l)],A_temp)\n",
    "#             a =  self.relu(z)\n",
    "#             A_temp =a\n",
    "            \n",
    "#         Z = np.dot(self.parameters[\"W\"+str(no_weights)],A_temp)\n",
    "#         a =  self.sigmoid(Z)\n",
    "\n",
    "#         A = self.sigmoid(A_temp)\n",
    "#         return A\n",
    "    \n",
    "#     def cost_function(self,Y,AL):\n",
    "#         # Y is the target\n",
    "#         # AL is the predicted output\n",
    "#         error = Y-AL\n",
    "#         #we will use mean square error\n",
    "#         cost = (1/2)*(error)**2\n",
    "#         return cost\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self,layer_list = [1,3,1]) -> None:\n",
    "        self.layer_list = layer_list\n",
    "        self.parameters = self.initialize_wights_deep(self.layer_list)\n",
    "\n",
    "    def sigmoid(self,x):\n",
    "        output = 1 / (1 + np.exp(-x))\n",
    "        return output  \n",
    "        \n",
    "    def relu(self,x):\n",
    "        output = np.maximum(0,x)\n",
    "        return output\n",
    "    \n",
    "    def initialize_wights_deep(self,layer_list):\n",
    "        parameters = {}\n",
    "        no_layers = len(layer_list)\n",
    "\n",
    "        for l in range(1, no_layers) :\n",
    "            parameters[\"W\" + str(l)] = np.random.randn(layer_list[l],layer_list[l-1]) * 0.01\n",
    "            parameters[\"b\" + str(l)] = np.random.randn(layer_list[l],1) * 0.01\n",
    "            \n",
    "        return parameters\n",
    "\n",
    "    def deep_forward(self,X):\n",
    "        no_weights = len(self.parameters)//2\n",
    "        A_temp = X\n",
    "        for l in range(1,no_weights):\n",
    "            z = np.dot(self.parameters[\"W\" + str(l)], A_temp) \n",
    "            a = self.relu(z)\n",
    "            A_temp = a\n",
    "        \n",
    "        Z = np.dot(self.parameters[\"W\" + str(no_weights)], A_temp)\n",
    "        A = self.sigmoid(Z)\n",
    "        return A\n",
    "\n",
    "    def cost_function(self,Y, AL) :\n",
    "        # Y is the target\n",
    "        # AL is the predicted output\n",
    "        error = Y - AL\n",
    "        # we will use mean square error\n",
    "        cost = (1/2) * (error)**2\n",
    "        return cost\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(layer_list=[3,4,5,6,7,8,9,1])\n",
    "\n",
    "X = np.random.randn(3,1)\n",
    "\n",
    "AL = model.deep_forward(X=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.125]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cost_function(Y=1 , AL= AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative of sigmoid function\n",
    "if f(x) is the sigmoid function then its derivative is given as;\n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\frac{d}{dx}\\sigma(x) =\\sigma(x)(1-\\sigma(x))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative of Relu function\n",
    "if f(x) is the relu function then its derivative is given as;\n",
    "\n",
    "$$\n",
    "f'(x) = f(x)\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider we have L layers in the network. Now for $l_{th}s layer, we have   \n",
    "\n",
    "$Z^{[l]} = W^{[l]} A^{[l-1]}+b^{[l]}$\n",
    "\n",
    "And then it is passed to the activation function. \n",
    "Let $g(x)$ be ther activation function of the $l_{th}$ layer then,  \n",
    "\n",
    "$A^{[l]} = g(z^{[l]}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the final layer, $A^{[l]}$ will be the predicted output. \n",
    "Then if we have to find the gradients of the weight, then we use chain rule to calculate it. For example:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{fn}}{\\partial w} = \\frac{\\partial L_{fn}}{\\partial A}* \\frac{\\partial A}{\\partial Z}* \\frac{\\partial A}{\\partial W}\n",
    "$$\n",
    "\n",
    "Let \n",
    "$$\n",
    "dZ^{[l]} = \\frac{\\partial L_{fn}}{\\partial Z^{[l]}} \\\\\n",
    "              = \\frac{\\partial L_{fn}}{\\partial A} *\\frac{\\partial A^{[l]}}{\\partial Z^{[l]}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus,\n",
    "\n",
    "$$\n",
    "dZ^{[l]}= d.A^{[l]} * g'^{[Z^{[l]}]}\n",
    "\n",
    "$$\n",
    "\n",
    "Now if we have $dZ^{[l]}$ , we can calculate others as:\n",
    "\n",
    "$$\n",
    "dW^{[l]} = \\frac{\\partial L_{fn}}{\\partial W^{[l]}} = dZ^{[l]} A^{[l-1] T}\n",
    "$$\n",
    "\n",
    "$$\n",
    "db^{[l]} = \\frac{\\partial L_{fn}}{\\partial b^{[l]}} = dZ^{[l] (i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dA^{[l-1]} = \\frac{\\partial L_{fn}}{\\partial A^{[l-1]}} = Z^{[l]T} dZ^{[l]}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, back propagation is being doing, each layers output value is needed. This is called as cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create two function, one for linear function and another for activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self,layer_list = [1,3,1]) -> None:\n",
    "        self.layer_list = layer_list\n",
    "        self.parameters = self.initialize_wights_deep(self.layer_list)\n",
    "        self.cache = []\n",
    "\n",
    "    def sigmoid(self,x):\n",
    "        output = 1 / (1 + np.exp(-x))\n",
    "        return output, x \n",
    "        \n",
    "    def relu(self,x):\n",
    "        output = np.maximum(0,x)\n",
    "        return output\n",
    "    \n",
    "    def initialize_wights_deep(self,layer_list):\n",
    "        parameters = {}\n",
    "        no_layers = len(layer_list)\n",
    "\n",
    "        for l in range(1, no_layers) :\n",
    "            parameters[\"W\" + str(l)] = np.random.randn(layer_list[l],layer_list[l-1]) * 0.01\n",
    "            parameters[\"b\" + str(l)] = np.random.randn(layer_list[l],1) * 0.01\n",
    "            \n",
    "        return parameters\n",
    "\n",
    "    def deep_forward(self,X):\n",
    "        no_weights = len(self.parameters)//2\n",
    "        A_temp = X\n",
    "        for l in range(1,no_weights):\n",
    "            z = np.dot(self.parameters[\"W\" + str(l)], A_temp) +self.parameters[\"b\"+str(l)]\n",
    "            linear_cache = (a,self.parameters[\"W\" + str(l)], self.parameters[\"b\" + str(l)])\n",
    "            a = self.relu(z)\n",
    "            A_temp = a\n",
    "            temp_cache = (a,z,self.parameters[\"W\" + str(l)], self.parameters[\"b\" + str(l)])\n",
    "            self.cache.append(temp_cache)\n",
    "\n",
    "            \n",
    "        \n",
    "        Z = np.dot(self.parameters[\"W\" + str(no_weights)], A_temp)\n",
    "        A = self.sigmoid(Z)\n",
    "        temp_cache = (A,Z,self.parameters[\"W\" + str(no_weights)], self.parameters[\"b\" + str(no_weights)])\n",
    "        self.cache.append()\n",
    "        return A\n",
    "\n",
    "    def cost_function(self,Y, AL) :\n",
    "        # Y is the target\n",
    "        # AL is the predicted output\n",
    "        error = Y - AL\n",
    "        # we will use mean square error\n",
    "        cost = (1/2) * (error)**2\n",
    "        return cost\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(layer_list = [3,4,1])\n",
    "x_input = np.random.randn(3,1)\n",
    "output = model.deep_forward(X = x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apeksha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
